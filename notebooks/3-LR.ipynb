{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model is similar to the linear regression model. However, in the logistic model the response variable $Y_{i}$ is binary. A binary variable takes two values, for example, $Y_{i} = 0$ and $Y_{i} = 1$, called \"failure\" and \"success\", respectively. In this case, \"success\" is the event of interest. \n",
    "\n",
    "We can't use a normal linear regression model on binary groups. It won't lead to a good fit. Instead we can transform a linear regression to a logistic regression curve. The Sigmoid (aka Logistic) Function takes in any value and outputs it to be between 0 and 1.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\phi(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This means we can take a linear regression solution and place it into the Sigmoid Function.\n",
    "\n",
    "$$\\begin{align}\n",
    "y=b_{0}+b_{1}x \\\\ \\\\ \n",
    "p=\\frac{1}{1+e^{(b_{0}+b_{1}x)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This results in a probability from 0 to 1 of belonging in the 1 class. We can set a cutoff point, at 0.5 for example, resulting that anything bellow it results in class 0, anything above it is class 1. We use the logistic function to output a value ranging from 0 to 1. Based on this probability we can assign a class.\n",
    "\n",
    "For futher information, please check Sections 4-4.3 of **Introduction to Statistical Learning** by Gareth James, et al.\n",
    "\n",
    "http://faculty.marshall.usc.edu/gareth-james/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/df_train.csv')\n",
    "df_test = pd.read_csv('../data/df_test.csv')\n",
    "\n",
    "X_train = df_train.drop('kill', axis=1)\n",
    "y_train = df_train['kill']\n",
    "X_test = df_test.drop(['kill'], axis=1)\n",
    "y_test = df_test['kill']\n",
    "\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130565, 11)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train-test split is a technique for evaluating the performance of a machine learning algorithm.\n",
    "\n",
    "It can be used for classification or regression problems and can be used for any supervised learning algorithm.\n",
    "\n",
    "The procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.\n",
    "\n",
    "- **Train Dataset:** Used to fit the machine learning model.\n",
    "- **Test Dataset:** Used to evaluate the fit machine learning model.\n",
    "\n",
    "The objective is to estimate the performance of the machine learning model on new data: data not used to train the model.\n",
    "\n",
    "The train-test procedure is appropriate when there is a sufficiently large dataset available. The train-test procedure is not appropriate when the dataset available is small. The reason is that when the dataset is split into train and test sets, there will not be enough data in the training dataset for the model to learn an effective mapping of inputs to outputs. There will also not be enough data in the test set to effectively evaluate the model performance. The estimated performance could be overly optimistic (good) or overly pessimistic (bad).\n",
    "\n",
    "If you have insufficient data, then a suitable alternate model evaluation procedure would be the **k-fold cross-validation procedure**.\n",
    "\n",
    "In addition to dataset size, another reason to use the train-test split evaluation procedure is computational efficiency.\n",
    "\n",
    "Samples from the original training dataset are split into the two subsets using random selection. This is to ensure that the train and test datasets are representative of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure has one main configuration parameter, which is the size of the train and test sets. This is most commonly expressed as a percentage between 0 and 1 for either the train or test datasets. \n",
    "\n",
    "There is no optimal split percentage.\n",
    "\n",
    "You must choose a split percentage that meets your project’s objectives with considerations that include:\n",
    "\n",
    "- Computational cost in training the model.\n",
    "- Computational cost in evaluating the model.\n",
    "- Training set representativeness.\n",
    "- Test set representativeness.\n",
    "\n",
    "Nevertheless, common split percentages include:\n",
    "\n",
    "- Train: 80%, Test: 20%\n",
    "- Train: 67%, Test: 33%\n",
    "- Train: 50%, Test: 50%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into training set and testing set using train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn Python machine learning library provides an implementation of the train-test split evaluation procedure via the train_test_split() function.\n",
    "\n",
    "The size of the split can be specified via the “test_size” argument that takes a number of rows (integer) or a percentage (float) of the size of the dataset between 0 and 1.\n",
    "\n",
    "Another important consideration is that rows are assigned to the train and test sets randomly.\n",
    "\n",
    "This is done to ensure that datasets are a representative sample (e.g. random sample) of the original dataset, which in turn, should be a representative sample of observations from the problem domain.\n",
    "\n",
    "When comparing machine learning algorithms, it is desirable (perhaps required) that they are fit and evaluated on the same subsets of the dataset.\n",
    "\n",
    "This can be achieved by fixing the seed for the pseudo-random number generator used when splitting the dataset. This can be achieved by setting the “random_state” to an integer value. Any value will do; it is not a tunable hyperparameter.\n",
    "\n",
    "Some classification problems do not have a balanced number of examples for each class label. As such, it is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset.\n",
    "\n",
    "This is called a stratified train-test split.\n",
    "\n",
    "We can achieve this by setting the “stratify” argument to the y component of the original dataset. This will be used by the train_test_split() function to ensure that both the train and test sets have the proportion of examples in each class that is present in the provided “y” array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We already have the the train and test datasets, so you are going to skip the train-test split part!**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = df.drop('kill', axis=1)\n",
    "y = df['kill'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=101, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization isn't required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and fitting a logistic regression model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Predicting values for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "\n",
    "The metrics that you choose to evaluate your machine learning algorithms are very important.\n",
    "\n",
    "Choice of metrics influences how the performance of machine learning algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose.\n",
    "\n",
    "Classification problems are perhaps the most common type of machine learning problem and as such there are a myriad of metrics that can be used to evaluate predictions for these problems.\n",
    "\n",
    "1. Classification Accuracy.\n",
    "2. Confusion Matrix.\n",
    "3. Precision-Recall Curves.\n",
    "4. Classification Report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Accuracy\n",
    "\n",
    "Classification accuracy is the number of correct predictions made as a ratio of all predictions made.\n",
    "\n",
    "This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.\n",
    "\n",
    "Classification accuracy is the total number of correct predictions divided by the total number of predictions made for a dataset.\n",
    "\n",
    "As a performance measure, accuracy is inappropriate for imbalanced classification problems.\n",
    "\n",
    "The main reason is that the overwhelming number of examples from the majority class (or classes) will overwhelm the number of examples in the minority class, meaning that even unskillful models can achieve accuracy scores of 90 percent, or 99 percent, depending on how severe the class imbalance happens to be. An alternative to using classification accuracy is to use precision and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.877\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is a handy presentation of the accuracy of a model with two or more classes.\n",
    "\n",
    "The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm.\n",
    "\n",
    "For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction=0 and actual=0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual=1. And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For imbalanced classification problems, the majority class is typically referred to as the negative outcome (e.g. such as “no change” or “negative test result“), and the minority class is typically referred to as the positive outcome (e.g. “change” or “positive test result”).\n",
    "\n",
    "The confusion matrix provides more insight into not only the performance of a predictive model, but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made.\n",
    "\n",
    "The simplest confusion matrix is for a two-class classification problem, with negative (class 0) and positive (class 1) classes.\n",
    "\n",
    "In this type of confusion matrix, each cell in the table has a specific and well-understood name, summarized as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "               | Positive Prediction | Negative Prediction\n",
    "Positive Class | True Positive (TP)  | False Negative (FN)\n",
    "Negative Class | False Positive (FP) | True Negative (TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20210    11]\n",
      " [ 2812     8]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall metrics are defined in terms of the cells in the confusion matrix, specifically terms like true positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall Curves\n",
    "\n",
    "**Precision for Imbalanced Classification**\n",
    "\n",
    "Precision is a metric that quantifies the number of correct positive predictions made.\n",
    "\n",
    "Precision, therefore, calculates the accuracy for the minority class.\n",
    "\n",
    "It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted.\n",
    "\n",
    "Precision evaluates the fraction of correct classified instances among the ones classified as positive … — Page 52, Learning from Imbalanced Data Sets, 2018.\n",
    "\n",
    "In an imbalanced classification problem with two classes, precision is calculated as the number of true positives divided by the total number of true positives and false positives."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision = True Positive (TP) / (True Positive (TP) + False Positive (FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a value between 0.0 for no precision and 1.0 for full or perfect precision.\n",
    "\n",
    "You can see that precision is simply the ratio of correct positive predictions out of all positive predictions made, or the accuracy of minority class predictions.\n",
    "\n",
    "Consider the same dataset, where a model predicts 50 examples belonging to the minority class, 45 of which are true positives and five of which are false positives. We can calculate the precision for this model as follows:\n",
    "\n",
    "This highlights that although precision is useful, it does not tell the whole story. It does not comment on how many real positive class examples were predicted as belonging to the negative class, so-called false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The precision score can be calculated using the *precision_score()* scikit-learn function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.421\n"
     ]
    }
   ],
   "source": [
    "# calculate prediction\n",
    "precision = precision_score(y_test,predictions, average='binary')\n",
    "print(f'Precision: {precision:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall for Imbalanced Classification**\n",
    "\n",
    "Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made.\n",
    "\n",
    "Unlike precision that only comments on the correct positive predictions out of all positive predictions, recall provides an indication of missed positive predictions.\n",
    "\n",
    "In this way, recall provides some notion of the coverage of the positive class.\n",
    "\n",
    "For imbalanced learning, recall is typically used to measure the coverage of the minority class. — Page 27, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n",
    "\n",
    "In an imbalanced classification problem with two classes, recall is calculated as the number of true positives divided by the total number of true positives and false negatives."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recall = True Positive (TP) / (True Positive (TP) + False Negative (FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a value between 0.0 for no recall and 1.0 for full or perfect recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The recall score can be calculated using the *recall_score()* scikit-learn function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.003\n"
     ]
    }
   ],
   "source": [
    "# calculate recall\n",
    "recall = recall_score(y_test,predictions, average='binary')\n",
    "print(f'Recall: {recall:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision vs. Recall for Imbalanced Classification**\n",
    "\n",
    "You may decide to use precision or recall on your imbalanced classification problem.\n",
    "\n",
    "Maximizing precision will minimize the number false positives, whereas maximizing the recall will minimize the number of false negatives.\n",
    "\n",
    "- Precision: Appropriate when minimizing false positives is the focus.\n",
    "- Recall: Appropriate when minimizing false negatives is the focus.\n",
    "\n",
    "Sometimes, we want excellent predictions of the positive class. We want high precision and high recall.\n",
    "\n",
    "This can be challenging, as often increases in recall often come at the expense of decreases in precision.\n",
    "\n",
    "In imbalanced datasets, the goal is to improve recall without hurting precision. These goals, however, are often conflicting, since in order to increase the TP for the minority class, the number of FP is also often increased, resulting in reduced precision. — Page 55, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n",
    "\n",
    "Nevertheless, instead of picking one measure or the other, we can choose a new metric that combines both precision and recall into one score.\n",
    "\n",
    "**F-Measure for Imbalanced Classification**\n",
    "\n",
    "Classification accuracy is widely used because it is one single measure used to summarize model performance.\n",
    "\n",
    "F-Measure provides a way to combine both precision and recall into a single measure that captures both properties.\n",
    "\n",
    "Alone, neither precision or recall tells the whole story. We can have excellent precision with terrible recall, or alternately, terrible precision with excellent recall. F-measure provides a way to express both concerns with a single score.\n",
    "\n",
    "Once precision and recall have been calculated for a binary or multiclass classification problem, the two scores can be combined into the calculation of the F-Measure.\n",
    "\n",
    "The traditional F measure is calculated as follows:\n",
    "\n",
    "* F-Measure = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "This is the harmonic mean of the two fractions. This is sometimes called the F-Score or the F1-Score and might be the most common metric used on imbalanced classification problems.\n",
    "\n",
    "\"*... the F1-measure, which weights precision and recall equally, is the variant most often used when learning from imbalanced data.*\" — Page 27, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n",
    "\n",
    "Like precision and recall, a poor F-Measure score is 0.0 and a best or perfect F-Measure score is 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The F-measure score can be calculated using the *f1_score()* scikit-learn function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.006\n"
     ]
    }
   ],
   "source": [
    "# calculate score\n",
    "f1 = f1_score(y_test, predictions, average='binary')\n",
    "print(f'F-Measure: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "Scikit-learn does provide a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures.\n",
    "\n",
    "The classification_report() function displays the precision, recall, f1-score and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93     20221\n",
      "           1       0.42      0.00      0.01      2820\n",
      "\n",
      "    accuracy                           0.88     23041\n",
      "   macro avg       0.65      0.50      0.47     23041\n",
      "weighted avg       0.82      0.88      0.82     23041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':np.logspace(-3,3,7), 'max_iter':[1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(),param_grid,refit=True,verbose=2, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'max_iter': [1000]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May take awhile!\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100.0, 'max_iter': 1000}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, max_iter=1000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid = grid.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8806188488492322"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the predictions and the inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 1.066 miliseconds\n"
     ]
    }
   ],
   "source": [
    "def calculate_pred_and_inf_time(best_grid, X_test):\n",
    "    # get the start time\n",
    "    st_wall_inf = time.time()\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    grid_predictions = best_grid.predict(X_test)\n",
    "\n",
    "    # get the end time\n",
    "    et_wall_inf = time.time()\n",
    "\n",
    "    # get execution time\n",
    "    wall_time_inf = et_wall_inf - st_wall_inf\n",
    "    print(f'Inference Time: {1000*wall_time_inf:.3f} miliseconds')\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20210    11]\n",
      " [ 2812     8]]\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "grid_predictions = best_grid.predict(X_test)\n",
    "print(confusion_matrix(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93     20221\n",
      "           1       0.42      0.00      0.01      2820\n",
      "\n",
      "    accuracy                           0.88     23041\n",
      "   macro avg       0.65      0.50      0.47     23041\n",
      "weighted avg       0.82      0.88      0.82     23041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_print(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)  \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n\")  \n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))  \n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))  \n",
    "    print(\"Accuracy: \", round(accuracy_score(y_test, y_pred),3))\n",
    "    print(\"Precision:\", round(precision_score(y_test, y_pred),3))\n",
    "    print(\"Recall:\", round(recall_score(y_test, y_pred),3))\n",
    "    print(\"f1: \", round(f1_score(y_test, y_pred),3))\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[20210    11]\n",
      " [ 2812     8]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93     20221\n",
      "           1       0.42      0.00      0.01      2820\n",
      "\n",
      "    accuracy                           0.88     23041\n",
      "   macro avg       0.65      0.50      0.47     23041\n",
      "weighted avg       0.82      0.88      0.82     23041\n",
      "\n",
      "Accuracy:  0.877\n",
      "Precision: 0.421\n",
      "Recall: 0.003\n",
      "f1:  0.006\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_and_print(best_grid,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({0: 114988, 1: 114988})\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[13168  7053]\n",
      " [  889  1931]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.65      0.77     20221\n",
      "           1       0.21      0.68      0.33      2820\n",
      "\n",
      "    accuracy                           0.66     23041\n",
      "   macro avg       0.58      0.67      0.55     23041\n",
      "weighted avg       0.85      0.66      0.71     23041\n",
      "\n",
      "Accuracy:  0.655\n",
      "Precision: 0.215\n",
      "Recall: 0.685\n",
      "f1:  0.327\n",
      "\n",
      "\n",
      "Inference Time: 1.126 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SMOTE(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({0: 114988, 1: 112138})\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[12937  7284]\n",
      " [  863  1957]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.64      0.76     20221\n",
      "           1       0.21      0.69      0.32      2820\n",
      "\n",
      "    accuracy                           0.65     23041\n",
      "   macro avg       0.57      0.67      0.54     23041\n",
      "weighted avg       0.85      0.65      0.71     23041\n",
      "\n",
      "Accuracy:  0.646\n",
      "Precision: 0.212\n",
      "Recall: 0.694\n",
      "f1:  0.325\n",
      "\n",
      "\n",
      "Inference Time: 1.097 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with ADASYN\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = ADASYN(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE and TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({0: 106266, 1: 106266})\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[13327  6894]\n",
      " [  915  1905]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.66      0.77     20221\n",
      "           1       0.22      0.68      0.33      2820\n",
      "\n",
      "    accuracy                           0.66     23041\n",
      "   macro avg       0.58      0.67      0.55     23041\n",
      "weighted avg       0.85      0.66      0.72     23041\n",
      "\n",
      "Accuracy:  0.661\n",
      "Precision: 0.217\n",
      "Recall: 0.676\n",
      "f1:  0.328\n",
      "\n",
      "\n",
      "Inference Time: 1.083 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE and TL\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SMOTETomek(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE and ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({1: 79510, 0: 69097})\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[12388  7833]\n",
      " [  814  2006]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.61      0.74     20221\n",
      "           1       0.20      0.71      0.32      2820\n",
      "\n",
      "    accuracy                           0.62     23041\n",
      "   macro avg       0.57      0.66      0.53     23041\n",
      "weighted avg       0.85      0.62      0.69     23041\n",
      "\n",
      "Accuracy:  0.625\n",
      "Precision: 0.204\n",
      "Recall: 0.711\n",
      "f1:  0.317\n",
      "\n",
      "\n",
      "Inference Time: 1.106 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE and ENN\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SMOTEENN(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work-asa]",
   "language": "python",
   "name": "conda-env-work-asa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
