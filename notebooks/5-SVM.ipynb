{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVM) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. They were extremely popular around the time they were developed in the 1990s and continue to be the go-to method for a high-performing algorithm with little tuning\n",
    "\n",
    "Given a set of training examples, each marked for belonging to one of two categories, and SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary classifier.\n",
    "\n",
    "An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. A hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1.\n",
    "\n",
    "The distance between the line and the closest data points is referred to as the margin. The best or optimal line that can separate the two classes is the line that as the largest margin. This is called the Maximal-Margin hyperplane. The margin is calculated as the perpendicular distance from the line to only the closest points. Only these points are relevant in defining the line and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane. The hyperplane is learned from training data using an optimization procedure that maximizes the margin.\n",
    "\n",
    "New examples are then mapped into the same space and predicted to belong to a category based on which side of the gap they fall on.\n",
    "\n",
    "The SVM algorithm is implemented in practice using a kernel.\n",
    "\n",
    "The learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra, which is out of the scope of this introduction to SVM.\n",
    "\n",
    "A powerful insight is that the linear SVM can be rephrased using the inner product of any two given observations, rather than the observations themselves. The inner product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "\n",
    "For example, the inner product of the vectors $[2, 3]$ and $[5, 6]$ is $2 * 5 + 3 * 6$ or $28$.\n",
    "\n",
    "The equation for making a prediction for a new input using the dot product between the input (x) and each support vector (xi) is calculated as follows:\n",
    "\n",
    "$f(x) = B_0 + \\sum(a_i \\dot (x,x_i))$\n",
    "\n",
    "This is an equation that involves calculating the inner products of a new input vector (x) with all support vectors in training data. The coefficients $B_0$ and $a_i$ (for each input) must be estimated from the training data by the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearnex import patch_sklearn \n",
    "patch_sklearn()\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import where\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/df_train.csv')\n",
    "df_test = pd.read_csv('../data/df_test.csv')\n",
    "\n",
    "X_train = df_train.drop('kill', axis=1)\n",
    "y_train = df_train['kill']\n",
    "X_test = df_test.drop(['kill'], axis=1)\n",
    "y_test = df_test['kill']\n",
    "\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Call the SVC() model from sklearn and fit the model to the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to train a Support Vector Machine Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Evaluations\n",
    "\n",
    "**Now get predictions from the model and create a confusion matrix and a classification report.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20086   135]\n",
      " [ 2434   386]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94     20221\n",
      "           1       0.74      0.14      0.23      2820\n",
      "\n",
      "    accuracy                           0.89     23041\n",
      "   macro avg       0.82      0.57      0.59     23041\n",
      "weighted avg       0.87      0.89      0.85     23041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_print(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)  \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))  \n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))  \n",
    "    print(\"Accuracy: \", round(accuracy_score(y_test, y_pred),3))\n",
    "    print(\"Precision:\", round(precision_score(y_test, y_pred),3))\n",
    "    print(\"Recall:\", round(recall_score(y_test, y_pred),3))\n",
    "    print(\"f1: \", round(f1_score(y_test, y_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[20086   135]\n",
      " [ 2434   386]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94     20221\n",
      "           1       0.74      0.14      0.23      2820\n",
      "\n",
      "    accuracy                           0.89     23041\n",
      "   macro avg       0.82      0.57      0.59     23041\n",
      "weighted avg       0.87      0.89      0.85     23041\n",
      "\n",
      "Accuracy:  0.889\n",
      "Precision: 0.741\n",
      "Recall: 0.137\n",
      "f1:  0.231\n"
     ]
    }
   ],
   "source": [
    "fit_and_print(model,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! Notice that we are classifying everything into a single class! This means our model needs to have it parameters adjusted (it may also help to normalize the data).\n",
    "\n",
    "We can search for parameters using a GridSearch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can tune the parameters to try to get even better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch\n",
    "\n",
    "Finding the right parameters (like what C or gamma values to use) is a tricky task! But luckily, we can be a little lazy and just try a bunch of combinations and see what works best! This idea of creating a 'grid' of parameters and just trying out all the possible combinations is called a Gridsearch, this method is common enough that Scikit-learn has this functionality built in with GridSearchCV! The CV stands for cross-validation which is the\n",
    "\n",
    "GridSearchCV takes a dictionary that describes the parameters that should be tried and a model to train. The grid of parameters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft Margin Classifier**\n",
    "\n",
    "In practice, real data is messy and cannot be separated perfectly with a hyperplane.\n",
    "\n",
    "The constraint of maximizing the margin of the line that separates the classes must be relaxed. This is often called the soft margin classifier. This change allows some points in the training data to violate the separating line.\n",
    "\n",
    "An additional set of coefficients are introduced that give the margin wiggle room in each dimension. These coefficients are sometimes called slack variables. This increases the complexity of the model as there are more parameters for the model to fit to the data to provide this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tuning parameter is introduced called simply $C$ that defines the magnitude of the wiggle allowed across all dimensions. The $C$ parameters defines the amount of violation of the margin allowed. A $C=0$ is no violation and we are back to the inflexible Maximal-Margin Classifier described above. The larger the value of $C$ the more violations of the hyperplane are permitted.\n",
    "\n",
    "During the learning of the hyperplane from data, all training instances that lie within the distance of the margin will affect the placement of the hyperplane and are referred to as support vectors. And as $C$ affects the number of instances that are allowed to fall within the margin, $C$ influences the number of support vectors used by the model.\n",
    "\n",
    "- The smaller the value of $C$, the more sensitive the algorithm is to the training data (higher variance and lower bias).\n",
    "- The larger the value of $C$, the less sensitive the algorithm is to the training data (lower variance and higher bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also have a more complex radial kernel. For example:\n",
    "\n",
    "\\begin{equation} K(x,x_{i}) = e^{- \\gamma\\sum(x – x_{i}^2)} \\end{equation}\n",
    "\n",
    "Where $\\gamma$ (gamma) is a parameter that must be specified to the learning algorithm. A good default value for gamma is 0.1, where gamma is often $0 < \\gamma < 1$. The radial kernel is very local and can create complex regions within the feature space, like closed polygons in two-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import GridsearchCV from SciKit Learn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great things about GridSearchCV is that it is a meta-estimator. It takes an estimator like SVC, and creates a new estimator, that behaves exactly the same - in this case, like a classifier. You should add refit=True and choose verbose to whatever number you want, higher the number, the more verbose (verbose just means the text output describing the process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=SVC(), param_grid=param_grid, verbose=2, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What fit does is a bit more involved then usual. First, it runs the same loop with cross-validation, to find the best parameter combination. Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation), to built a single new model using the best parameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now take that grid model and create some predictions using the test set and create classification reports and confusion matrices for them. Were we able to improve?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 5.5min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 8.1min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 2.3min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 9.4min\n",
      "[CV] END ...................C=1000, gamma=0.0001, kernel=rbf; total time= 1.9min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 2.9min\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time= 1.8min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 9.5min\n",
      "[CV] END ....................C=0.1, gamma=0.0001, kernel=rbf; total time=  22.5s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time= 2.4min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 9.5min\n",
      "[CV] END ...................C=1000, gamma=0.0001, kernel=rbf; total time= 2.5min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  29.5s\n",
      "[CV] END ......................C=1, gamma=0.0001, kernel=rbf; total time=  25.3s\n",
      "[CV] END ......................C=1, gamma=0.0001, kernel=rbf; total time= 1.1min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=12.8min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 5.3min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 9.9min\n",
      "[CV] END ....................C=0.1, gamma=0.0001, kernel=rbf; total time=  19.4s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time= 1.9min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=13.4min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 2.9min\n",
      "[CV] END .....................C=10, gamma=0.0001, kernel=rbf; total time= 1.7min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 8.2min\n",
      "[CV] END ...................C=1000, gamma=0.0001, kernel=rbf; total time= 2.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 5.4min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 8.1min\n",
      "[CV] END ...................C=1000, gamma=0.0001, kernel=rbf; total time= 2.2min\n",
      "[CV] END ....................C=0.1, gamma=0.0001, kernel=rbf; total time=  19.8s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time= 2.4min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=10.3min\n",
      "[CV] END ...................C=1000, gamma=0.0001, kernel=rbf; total time= 2.6min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 3.1min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=15.8min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 3.3min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=15.8min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 3.3min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=15.9min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 3.5min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=16.0min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 3.3min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=16.4min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  35.7s\n",
      "[CV] END ......................C=1, gamma=0.0001, kernel=rbf; total time=  36.8s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 9.2min\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=17.5min\n",
      "[CV] END ....................C=0.1, gamma=0.0001, kernel=rbf; total time=  28.6s\n",
      "[CV] END ......................C=1, gamma=0.0001, kernel=rbf; total time=  42.4s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 9.8min\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=18.0min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 3.1min\n",
      "[CV] END .....................C=10, gamma=0.0001, kernel=rbf; total time= 1.9min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 5.5min\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=17.1min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  28.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time= 2.5min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 4.6min\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=21.8min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 2.3min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 4.4min\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=22.1min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 2.9min\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time= 2.6min\n",
      "[CV] END ....................C=100, gamma=0.0001, kernel=rbf; total time= 3.3min\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=19.9min\n",
      "[CV] END ....................C=0.1, gamma=0.0001, kernel=rbf; total time=  19.8s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time= 2.0min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 9.7min\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=17.4min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 2.3min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 4.6min\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=22.2min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 3.0min\n",
      "[CV] END .....................C=10, gamma=0.0001, kernel=rbf; total time= 1.9min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 5.0min\n",
      "[CV] END ....................C=1000, gamma=0.001, kernel=rbf; total time=18.9min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 2.4min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 4.4min\n",
      "[CV] END ........................C=1000, gamma=1, kernel=rbf; total time=21.3min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 4.2min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=25.7min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 4.0min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=25.8min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 3.9min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=27.1min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 4.1min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=27.0min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 4.1min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=27.1min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time= 2.1min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 4.4min\n",
      "[CV] END .....................C=1000, gamma=0.01, kernel=rbf; total time=21.6min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 3.1min\n",
      "[CV] END .....................C=10, gamma=0.0001, kernel=rbf; total time= 2.0min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 4.3min\n",
      "[CV] END .....................C=1000, gamma=0.01, kernel=rbf; total time=21.7min\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  38.3s\n",
      "[CV] END ......................C=1, gamma=0.0001, kernel=rbf; total time= 1.1min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 8.0min\n",
      "[CV] END .....................C=1000, gamma=0.01, kernel=rbf; total time=21.9min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 5.7min\n",
      "[CV] END ....................C=100, gamma=0.0001, kernel=rbf; total time= 2.8min\n",
      "[CV] END .....................C=1000, gamma=0.01, kernel=rbf; total time=22.3min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 2.8min\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time= 6.2min\n",
      "[CV] END .....................C=1000, gamma=0.01, kernel=rbf; total time=22.3min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         'kernel': ['rbf']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May take awhile!\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the best parameters found by GridSearchCV in the best_params_ attribute, and the best estimator in the best\\_estimator_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid = grid.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can re-run predictions on this grid object just like you would with a normal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20014   207]\n",
      " [ 2298   522]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94     20221\n",
      "           1       0.72      0.19      0.29      2820\n",
      "\n",
      "    accuracy                           0.89     23041\n",
      "   macro avg       0.81      0.59      0.62     23041\n",
      "weighted avg       0.87      0.89      0.86     23041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[20014   207]\n",
      " [ 2298   522]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94     20221\n",
      "           1       0.72      0.19      0.29      2820\n",
      "\n",
      "    accuracy                           0.89     23041\n",
      "   macro avg       0.81      0.59      0.62     23041\n",
      "weighted avg       0.87      0.89      0.86     23041\n",
      "\n",
      "Accuracy:  0.891\n",
      "Precision: 0.716\n",
      "Recall: 0.185\n",
      "f1:  0.294\n"
     ]
    }
   ],
   "source": [
    "fit_and_print(best_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 266.469 miliseconds\n"
     ]
    }
   ],
   "source": [
    "def calculate_pred_and_inf_time(best_grid, X_test):\n",
    "    # get the start time\n",
    "    st_wall_inf = time.time()\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    grid_predictions = best_grid.predict(X_test)\n",
    "\n",
    "    # get the end time\n",
    "    et_wall_inf = time.time()\n",
    "\n",
    "    # get execution time\n",
    "    wall_time_inf = et_wall_inf - st_wall_inf\n",
    "    print(f'Inference Time: {1000*wall_time_inf:.3f} miliseconds')\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have done about the same or exactly the same, this makes sense, there is basically just one point that is too noisey to grab, which makes sense, we don't want to have an overfit model that would be able to grab that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "Support Vector Machines are a huge area of study. There are numerous books and papers on the topic. Here let's list some of the seminal and most useful results if you are looking to dive deeper into the background and theory of the technique.\n",
    "\n",
    "- Check Chapter 9 of **Introduction to Statistical Learning** by Gareth James, et al.\n",
    "\n",
    "http://faculty.marshall.usc.edu/gareth-james/\n",
    "\n",
    "There are countless tutorials and journal articles on SVM. Below is a link to a seminal paper on SVM by Cortes and Vapnik and another to an excellent introductory tutorial.  \n",
    "\n",
    "- Support-Vector Networks by Cortes and Vapnik 1995\n",
    "\n",
    "https://link.springer.com/article/10.1007/BF00994018\n",
    "- A Tutorial on Support Vector Machines for Pattern Recognition 1998\n",
    "\n",
    "https://www.di.ens.fr/~mallat/papiers/svmtutorial.pdf\n",
    "\n",
    "\n",
    "Finally, there are a lot of posts on Q&A sites asking for simple explanations of SVM, below are two picks that you might find useful.\n",
    "\n",
    "- What does support vector machine (SVM) mean in layman’s terms?\n",
    "\n",
    "https://www.quora.com/What-does-support-vector-machine-SVM-mean-in-laymans-terms\n",
    "- Please explain Support Vector Machines (SVM) like I am a 5 year old\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({0: 114988, 1: 114988})\n",
      "Confusion Matrix: \n",
      " [[15591  4630]\n",
      " [  764  2056]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85     20221\n",
      "           1       0.31      0.73      0.43      2820\n",
      "\n",
      "    accuracy                           0.77     23041\n",
      "   macro avg       0.63      0.75      0.64     23041\n",
      "weighted avg       0.87      0.77      0.80     23041\n",
      "\n",
      "Accuracy:  0.766\n",
      "Precision: 0.308\n",
      "Recall: 0.729\n",
      "f1:  0.433\n",
      "Inference Time: 654.650 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SMOTE(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({1: 119427, 0: 114988})\n",
      "Confusion Matrix: \n",
      " [[14423  5798]\n",
      " [  605  2215]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82     20221\n",
      "           1       0.28      0.79      0.41      2820\n",
      "\n",
      "    accuracy                           0.72     23041\n",
      "   macro avg       0.62      0.75      0.61     23041\n",
      "weighted avg       0.88      0.72      0.77     23041\n",
      "\n",
      "Accuracy:  0.722\n",
      "Precision: 0.276\n",
      "Recall: 0.785\n",
      "f1:  0.409\n",
      "Inference Time: 752.793 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with ADASYN\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = ADASYN(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE and TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({0: 111748, 1: 111748})\n",
      "Confusion Matrix: \n",
      " [[15597  4624]\n",
      " [  769  2051]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85     20221\n",
      "           1       0.31      0.73      0.43      2820\n",
      "\n",
      "    accuracy                           0.77     23041\n",
      "   macro avg       0.63      0.75      0.64     23041\n",
      "weighted avg       0.87      0.77      0.80     23041\n",
      "\n",
      "Accuracy:  0.766\n",
      "Precision: 0.307\n",
      "Recall: 0.727\n",
      "f1:  0.432\n",
      "Inference Time: 614.533 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE and TL\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SMOTETomek(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE and ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 114988, 1: 15577})\n",
      "Counter({1: 96382, 0: 81992})\n",
      "Confusion Matrix: \n",
      " [[14785  5436]\n",
      " [  635  2185]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83     20221\n",
      "           1       0.29      0.77      0.42      2820\n",
      "\n",
      "    accuracy                           0.74     23041\n",
      "   macro avg       0.62      0.75      0.62     23041\n",
      "weighted avg       0.88      0.74      0.78     23041\n",
      "\n",
      "Accuracy:  0.737\n",
      "Precision: 0.287\n",
      "Recall: 0.775\n",
      "f1:  0.419\n",
      "Inference Time: 321.345 miliseconds\n"
     ]
    }
   ],
   "source": [
    "# Oversample and plot imbalanced dataset with SMOTE and ENN\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y_train)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample = SMOTEENN(random_state=42)\n",
    "X_train_rel, y_train_rel = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_train_rel)\n",
    "print(counter)\n",
    "\n",
    "fit_and_print(best_grid, X_train_rel, y_train_rel)\n",
    "\n",
    "calculate_pred_and_inf_time(best_grid, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work-asa]",
   "language": "python",
   "name": "conda-env-work-asa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
